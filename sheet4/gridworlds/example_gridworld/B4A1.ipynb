{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gridworlds           # import to trigger registration of the environment\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# create instance\n",
    "env = gym.make(\"gridworld-v0\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self, exploration_factor = 0.1, gamma=0.9, initial_policy=None):\n",
    "        \n",
    "        self.exploration_factor = exploration_factor\n",
    "        self.gamma=gamma\n",
    "\n",
    "        if initial_policy ==None:\n",
    "           self.policy = np.full((5, 5, 4), 0.25)\n",
    "           #self.policy = np.tile([0.1, 0.1, 0.4, 0.4], (16, 1))\n",
    "     \n",
    "        else:\n",
    "            self.policy=initial_policy\n",
    "       \n",
    "        self.N_detailed=np.zeros((16,4,16))\n",
    "        self.Q=np.zeros((16,4))\n",
    "        self.v=np.zeros((16))\n",
    "        self.Returns=[[] for i in range(16)]\n",
    "        self.Rewards_cum=np.zeros((16,4))\n",
    "        self.pos_dict={i*4+j: [i,j] for i in range(4) for j in range(4)}\n",
    "\n",
    "    def act(self, state):\n",
    "        # epsilon-greedy\n",
    "        if np.random.rand() < self.exploration_factor:\n",
    "            return np.random.choice(4) # random action\n",
    "        else:\n",
    "            return np.argmax(self.policy[state[0], state[1]]) # greedy action\n",
    "    \n",
    "    \n",
    "    def update(self, state, action):\n",
    "        self.policy[state[0], state[1]] = np.zeros(4)\n",
    "        self.policy[state[0], state[1], action] = 1\n",
    "    \n",
    "    \n",
    "    def eval_episode(self,episode):\n",
    "        states, actions, rewards= episode \n",
    "        next_state=states[-1]\n",
    "        G=0\n",
    "        for i, state in reversed(list(enumerate(states[:-1]))):\n",
    "            action=actions[i]\n",
    "            reward=rewards[i]\n",
    "            self.Rewards_cum[state,action]+=reward\n",
    "            G=self.gamma*G + reward\n",
    "            self.N_detailed[state,action,next_state]+=1\n",
    "            if state not in states[:i]:\n",
    "                self.Returns[state].append(G)\n",
    "                self.v[state]=sum(self.Returns[state])/len(self.Returns[state])\n",
    "            next_state=state\n",
    "\n",
    "\n",
    "    def compute_q(self):\n",
    "        for state in range(self.Q.shape[0]):\n",
    "            for action in range(self.Q.shape[1]):\n",
    "\n",
    "                s_a_visits=np.sum(self.N_detailed[state,action])\n",
    "                if s_a_visits==0:\n",
    "                    s_a_visits=1 # dirty solution to avoiding nan values\n",
    "                probs_s_a=self.N_detailed[state,action]/s_a_visits\n",
    "                s_a_rewards=self.Rewards_cum[state,action]/s_a_visits\n",
    "                self.Q[state,action]=np.dot(probs_s_a,self.v)+s_a_rewards\n",
    "\n",
    "    def improve_policy(self):\n",
    "        self.compute_q()\n",
    "        for state in range(self.Q.shape[0]):\n",
    "            best_action_value=np.max(self.Q[state])\n",
    "            base=0\n",
    "            new_probs=np.zeros(4)\n",
    "            for i, action_value in enumerate(self.Q[state]):\n",
    "                if action_value== best_action_value:\n",
    "                    new_probs[i]=1\n",
    "                    base+=1\n",
    "            new_probs=new_probs/base\n",
    "            self.policy[state] = new_probs\n",
    "        self.Returns=[[] for i in range(16)]\n",
    "    \n",
    "    def optimise(self,env,max_iterations, eps_per_iter=100,max_eps_length=100):\n",
    "        for it in range(max_iterations):\n",
    "            for eps_nr in range(eps_per_iter):\n",
    "                env.reset()\n",
    "                state, _ = env.reset()\n",
    "                episode=[[state],[],[]]\n",
    "                for i in range(max_eps_length):\n",
    "                    action = self.act(state)\n",
    "                    state, reward, terminated, truncated, _ = env.step(action) \n",
    "                    episode[0].append(state)\n",
    "                    episode[1].append(action)\n",
    "                    episode[2].append(reward)\n",
    "                    \n",
    "                    if terminated or truncated:\n",
    "                        break\n",
    "                \n",
    "                self.eval_episode(episode)\n",
    "            \n",
    "            if it % 10 == 0:\n",
    "                self.improve_policy()\n",
    "                self.show_heatmap(it)\n",
    "                self.show_policy(it)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_gridworld(final_value, final_policy):\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(final_value, cmap='viridis')\n",
    "\n",
    "    # Show all ticks and label them with the respective list entries\n",
    "    ax.set_xticks(np.arange(5))\n",
    "    ax.set_yticks(np.arange(5))\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            text = ax.text(j, i, round(final_value[i, j], 2),\n",
    "                        ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    ax.set_title(\"5x5 Gridworld Value Function\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    policy_arrows = {0: '↑', 1: '→', 2: '↓', 3: '←'}\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(final_value, cmap='viridis')\n",
    "\n",
    "    # Show all ticks and label them with the respective list entries\n",
    "    ax.set_xticks(np.arange(5))\n",
    "    ax.set_yticks(np.arange(5))\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            best_action = np.argmax(final_policy[i, j])\n",
    "            text = ax.text(j, i, policy_arrows[best_action],\n",
    "                        ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    ax.set_title(\"5x5 Gridworld Optimal Policy\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
